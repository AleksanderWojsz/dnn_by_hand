{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-27T06:26:15.217368Z",
     "start_time": "2024-10-27T06:26:15.214129Z"
    }
   },
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T06:26:15.680914Z",
     "start_time": "2024-10-27T06:26:15.222380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "permutation = np.random.permutation(len(x_train))\n",
    "x_train = x_train[permutation]\n",
    "y_train = y_train[permutation]\n",
    "\n",
    "def vectorize(list, vector_len):\n",
    "    result = np.zeros((len(list), vector_len))\n",
    "    for i in range(len(list)):\n",
    "        result[i, list[i]] = 1\n",
    "    return result\n",
    "\n",
    "def normalize(matrix):\n",
    "    min_value = np.min(matrix, axis=1).reshape(-1, 1)\n",
    "    max_value = np.max(matrix, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return (matrix - min_value) / (max_value - min_value)\n",
    "\n",
    "y_train = vectorize(y_train, 10)\n",
    "y_test = vectorize(y_test, 10)\n",
    "\n",
    "x_train = normalize(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test = normalize(x_test.reshape(x_test.shape[0], -1))\n"
   ],
   "id": "f4e9388cc29a3be1",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T06:26:15.703248Z",
     "start_time": "2024-10-27T06:26:15.698427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Shapes:\")\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ],
   "id": "34beb892984def3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "x_train (60000, 784)\n",
      "y_train (60000, 10)\n",
      "x_test (10000, 784)\n",
      "y_test (10000, 10)\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T06:27:11.580385Z",
     "start_time": "2024-10-27T06:26:15.765163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(x_train, y_train, x_test, y_test, layers, epochs=10, learning_rate=0.01, batch_size=200):\n",
    "    \n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l1, l2 in zip(layers[:-1], layers[1:]):\n",
    "       weights.append(np.random.randn(l1, l2) / 10)\n",
    "       biases.append(np.zeros(l2))\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        number_of_batches = x_train.shape[0] // batch_size\n",
    "        x_batches = [x_train[i * batch_size: (i + 1) * batch_size] for i in range(number_of_batches)]\n",
    "        y_batches = [y_train[i * batch_size: (i + 1) * batch_size] for i in range(number_of_batches)]\n",
    "        for x_batch, y_batch in tqdm(zip(x_batches, y_batches), total=len(x_batches)):\n",
    "            \n",
    "            biases_gradients_sum = [np.zeros(b.shape) for b in biases]\n",
    "            weights_gradients_sum = [np.zeros(w.shape) for w in weights]\n",
    "            for x, y in zip(x_batch, y_batch):\n",
    "                predictions, layer_input, before_act = forward_propagation(x, weights, biases)\n",
    "                b_gradients, w_gradients = backward_propagation(predictions, y, weights, biases, layer_input, before_act)\n",
    "                biases_gradients_sum = [sum + new_gradient for sum, new_gradient in zip(biases_gradients_sum, b_gradients)]\n",
    "                weights_gradients_sum = [sum + new_gradient for sum, new_gradient in zip(weights_gradients_sum, w_gradients)]\n",
    "    \n",
    "            biases = [b - learning_rate * nb / batch_size for b, nb in zip(biases, biases_gradients_sum)]\n",
    "            weights = [w - learning_rate * nw / batch_size for w, nw in zip(weights, weights_gradients_sum)]\n",
    "            \n",
    "        print(\"test\", test(x_test, y_test, weights, biases))\n",
    "            \n",
    "    return biases, weights\n",
    "\n",
    "def test(x_matrix, y_matrix, weights, biases):\n",
    "    correct_answers = 0\n",
    "    for x, y in zip(x_matrix, y_matrix):\n",
    "        predictions, _, _ = forward_propagation(x, weights, biases)\n",
    "        if y[np.argmax(predictions)] == 1:\n",
    "            correct_answers += 1\n",
    "        \n",
    "    return str(correct_answers / y_matrix.shape[0] * 100) + \"%\"\n",
    "    \n",
    "def forward_propagation(x, weights, biases):\n",
    "    layer_input = []\n",
    "    before_act = []\n",
    "    for w, b in zip(weights, biases):\n",
    "        layer_input.append(x)\n",
    "        x = x @ w + b\n",
    "        before_act.append(x)\n",
    "        x = sigmoid(x)\n",
    "        \n",
    "    return x, layer_input, before_act\n",
    "   \n",
    "def backward_propagation(predictions, y, weights, biases, layer_input, before_act):\n",
    "    \n",
    "    b_gradients = []\n",
    "    w_gradients = []\n",
    "    \n",
    "    diff = (2 / y.shape[0]) * (predictions - y)\n",
    "    for linp, bef_act, w, b in reversed(list(zip(layer_input, before_act, weights, biases))):\n",
    "        diff = sigmoid_derivative(bef_act) * diff\n",
    "        b_gradients.append(diff.flatten())\n",
    "        w_gradients.append(linp.reshape(-1, 1) @ diff.reshape(1, -1))\n",
    "        diff = diff @ w.T\n",
    "\n",
    "    return list(reversed(b_gradients)), list(reversed(w_gradients))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "biases, weights = train(x_train, y_train, x_test, y_test, layers=[784, 32, 10], epochs=10, learning_rate=3, batch_size=100)"
   ],
   "id": "8e1cd996460fd16a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 112.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 82.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 105.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 89.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 116.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 90.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 114.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 90.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 111.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 91.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 105.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 92.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 108.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 92.36999999999999%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 112.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 92.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 107.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 92.82000000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:05<00:00, 116.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 92.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 126
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
